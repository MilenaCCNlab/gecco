loop:
  max_iterations: 5

task:
  name: "multi_attribute_decision_making"
  description: |
    This is a multi-attribute decision-making task where participants choose the superior product between two options, A and B.
    Each option represents a fictitious product, and participants must infer which is superior in quality on each trial.
    For each option, four expert ratings are provided, with 1 indicating a positive rating and 0 indicating a negative rating.
    The experts differ in validity, presented in descending order: 0.9, 0.8, 0.7, and 0.6.
    Participants make their choice by selecting either option A or B.

  goal: |
    Your task: Propose **{models_per_iteration} new cognitive models** as Python functions:
    {model_names}

#  instructions: "Participants choose between two spaceships and interact with aliens for rewards."

data:
  max_prompt_trials: 5
  path: "data/multi_attribute_decision_making.csv"
  id_column: "participant"
  input_columns: ["A_feature1", "A_feature2", "A_feature3", "A_feature4","B_feature1","B_feature2","B_feature3","B_feature4","decisions"]
  data2text_function: "narrative"

  narrative_template: |
    Option A features: {A_feature1}, {A_feature2}, {A_feature3}, {A_feature4};
    Option B features: {B_feature1}, {B_feature2}, {B_feature3}, {B_feature4};
    Choice: {decisions}

#  value_mappings:
#    choice_1:
#      "0": "A"
#      "1": "U"
#    state:
#      "0": "X"
#      "1": "Y"
#    choice_2:
#      "0": "W"
#      "1": "S"

  splits:
    prompt: "first3"
    eval: "next10"
    test: "remainder"

llm:
  provider: "openai"
  base_model: "gpt-5"
  temperature: 0.2
  max_tokens: 2048
  max_output_tokens: 2048
  reasoning_effort: "medium"
  text_verbosity: "low"       # üëà new field
  api_key: null
  system_prompt: |
    You are a renowned cognitive scientist and expert Python programmer.
    You will be given several datasets from a multi-attribute decision-making task.
    Your objective is to write candidate cognitive models, expressed as Python functions, that explain the underlying task-solving processes reflected in the data.

  models_per_iteration: 3
  include_feedback: true             # ‚¨ÖÔ∏è whether to include feedback in later iterations
  guardrails:
    - "Each model must be a standalone Python function."
    - "Function names must be `cognitive_model1`, `cognitive_model2`, etc."
    - "Take as input: `decisions, A_feature1, A_feature2, A_feature3, A_feature4, B_feature1, B_feature2, B_feature3, B_feature4, parameters`."
    - "Return the **negative log-likelihood** of observed choices."
    - "Use all parameters meaningfully (no unused params)."
    - "Include a clear docstring for the model and each parameter."
    - "Parameter bounds: [0,1] for most; [0,10] for inverse temperature. Please refer to the template for how these are defined."
    - "Do NOT include any package imports inside the code you write. Assume all packages are already imported."

  template_model: |
    def cognitive_model(decisions, A_feature1, A_feature2, A_feature3, A_feature4, B_feature1, B_feature2, B_feature3, B_feature4, parameters):
        """Example model illustrating format only (do not reuse logic).
        Bounds:
        weight1: [0,1]
        weight2: [0,1]
        weight3: [0,1]
        weight4: [0,1]
        temperature: [0,10]
        """
        weight1, weight3, weight3, weights4, temperature = parameters
        validities = [0.9,0.8,0.7,0.6]
        n_trials = len(decisions)

        log_likelihood = 0
    
        for trial in range(n_trials):
            option_A, option_B = [A_feature1[t],A_feature2[t],A_feature3[t],A_feature4[t]], [B_feature1[t],B_feature2[t],B_feature3[t],B_feature4[t]]
            value_A = np.array(option_A)
            value_B = np.array(option_B)
            scale_value_difference = temperature * np.sum(value_B - value_A)
            choice_probability_B = 1.0 / (1.0 + np.exp(-scale_value_difference))
            p = decisions[t] * np.log(choice_probability_B) + (1 - choices[t]) * np.log(1 - choice_probability_B)
            log_likelihood += p
    
        return log_likelihood

evaluation:
  metric: "bic"
  optimizer: "L-BFGS-B"

feedback:
  type: "manual"   # or "llm"
