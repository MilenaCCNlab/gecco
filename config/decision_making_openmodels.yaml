loop:
  max_iterations: 5

task:
  name: "multi_attribute_decision_making"
  description: |
    This is a multi-attribute decision-making task, where participants have to choose the superior product between two options, labeled A and B.
    Each option represented a fictitious product and they had to infer which product was superior in terms of quality in every trial.
    For each option, they were provided with four expert ratings (with 1 representing a positive and 0 representing a negative rating).
    The four experts differ in their validity.
    The ratings of experts were given in descending order of their validity (having validities of 0.9, 0.8, 0.7, and 0.6 respectively).
    Participants selected a product by selecting the corresponding option, i.e. A or B.


  goal: |
     Propose **{models_per_iteration} unique cognitive models** that could explain the observed behavior in the dataset
     Each model should have distinct assumptions and parameters. Avoid repeating ideas used in previous iterations.
     Think step by step: How do participants use features to or expert validites to make their decisions?
     Each model should be implemented as a Python function named: {model_names}
     Each function should take following inputs: a list of choices, A feature values, B feature values as lists, and a list of model parameters (see the in template below).
     Each function should return the negative log likelihood of the observed choices given its parameters.
     Please also specify the model parameter boundaries in the function docstring as shown in the code template below.

   

data:
  max_prompt_trials: 5
  path: "/u/ajagadish/hcai_hackathon_2024/icml_rebuttals/contamination_for_akshay/multi_attribute/hilbig_exp1.csv" #"data/multi_attribute_decision_making.csv"
  id_column: "participant"
  input_columns: ["choice", "stimulus_0", "stimulus_1"]
  data2text_function: "narrative"

  narrative_template: |
    Trial {trial}: Product A ratings: {stimulus_0}. Product B ratings: {stimulus_1}. Chosen option: {choice}.

  splits: ## not a big fan
    prompt: "first3"
    eval: "next5"
    test: "remainder"

llm:
  provider: "llama"
  base_model: "meta-llama/Meta-Llama-3.1-70B-Instruct"
  temperature: 0.2
  top_p: 0.8
  max_tokens: 2048
  # max_output_tokens: 2048
  # reasoning_effort: "medium"
  # text_verbosity: "low"       # üëà new field
  system_prompt: |
    You are a famous cognitive scientist and an expert programmer.
    You will be provided with several datasets from multi-attribute decision-making task.
    Your objective is to write candidate cognitive models that could explain the underlying task-solving process in the data in the format of Python functions.


  models_per_iteration: 3
  include_feedback: true             # ‚¨ÖÔ∏è whether to include feedback in later iterations
  guardrails:
    - "Clearly define each model parameter and explain its role in the function's commented section."
    - "All parameters (except inverse temperature) should have values between 0 and 1."
    - "Ensure the equations do not result in nonsense values (e.g., avoid division by zero)."
    - "Make sure your Python functions are executable and bug-free."

  template_model: |
    def cognitive_model(decisions, A_feature1, A_feature2, A_feature3, A_feature4, B_feature1, B_feature2, B_feature3, B_feature4, parameters):
        '''
        Input:
            choices - participant choices for all trials (numpy array)
            optionA_f1-optionA_f4: - four expert ratings for option A for all trials (numpy array)
            optionB_f1-optionB_f4 - four expert ratings for option B for all trials (numpy array)
            validities - validities for the four expert ratings, in the same order listed in the options (numpy array)
            parameters - list of parameters
    
        Output:
            negative log likelihood - negative log likelihood of choices conditioned on model parameters
    
        Bounds:
        weight1: [0,1]
        weight2: [0,1]
        weight3: [0,1]
        weight4: [0,1]
        temperature: [0,10]

        '''
        weight1, weight3, weight3, weights4, temperature = parameters
        validities = [0.9,0.8,0.7,0.6]
        n_trials = len(decisions)

        log_likelihood = 0
    
        for trial in range(n_trials):
            option_A, option_B = [A_feature1[t],A_feature2[t],A_feature3[t],A_feature4[t]], [B_feature1[t],B_feature2[t],B_feature3[t],B_feature4[t]]
            value_A = np.array(option_A)
            value_B = np.array(option_B)
            scale_value_difference = temperature * np.sum(value_B - value_A)
            choice_probability_B = 1.0 / (1.0 + np.exp(-scale_value_difference))
            p = decisions[t] * np.log(choice_probability_B) + (1 - choices[t]) * np.log(1 - choice_probability_B)
            log_likelihood += np.log(p)

        return -log_likelihood

evaluation:
  metric: "bic"
  optimizer: "L-BFGS-B"

feedback:
  # type: "manual" # or "llm"
  # prompt: |
  #   Your best model so far:

  #   {best_model}.

  #   These are parameter combinations tried so far:
  #   {previous_parameters}.

  #   Avoid repeating these exact combinations and explore alternative parameter configurations or mechanisms.
  type: "llm"
  prompt: |
    The best model so far was:

    {best_model}.

    The following parameter combinations have already been explored:
    {previous_parameters}.

    Please suggest high-level guidance for generating new model variants that differ conceptually but might still perform well.