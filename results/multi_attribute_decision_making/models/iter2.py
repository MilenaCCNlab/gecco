def cognitive_model1(decisions, A_feature1, A_feature2, A_feature3, A_feature4,
                     B_feature1, B_feature2, B_feature3, B_feature4, parameters):
    """Stochastic Take-The-Best (TTB) / Compensatory mixture with cue misperception, search order, aspiration, bias, and lapse.

    Cognitive idea:
    - With probability p_ttb, the decision maker uses a noisy lexicographic (TTB) rule:
      They inspect cues sequentially until finding the first discriminating cue; the side it favors determines choice.
      Cues can be misperceived (flipped) with probability flip.
      Search starts from the most valid cue with probability start_high, otherwise from least valid upward.
      An aspiration parameter asp scales the strength of the first discriminating cue.
    - With probability (1 - p_ttb), they use a compensatory validity-weighted additive rule.
    - Choices are generated by a softmax on the decision variable (B minus A), with a baseline side bias and lapse.

    Parameters (in order):
    - p_ttb: [0,1] Mixture weight; probability of using the TTB process on a given trial.
    - flip: [0,1] Probability that each cue is misperceived (0 -> accurate, 1 -> fully random flip each cue).
    - start_high: [0,1] Probability of starting the TTB scan from the highest-validity cue (else from the lowest).
    - asp: [0,1] Aspiration scaling for the discriminating cue in TTB; scales the effective DV magnitude from the first separator.
    - biasB: [0,1] Baseline side bias toward B (0.5 = no bias); internally centered to [-1,1].
    - lapse: [0,1] Probability of random choice (uniform 0.5), mixed with the model-based probability.
    - beta: [0,10] Inverse temperature for the softmax.

    Inputs:
    - decisions: array-like of 0/1 where 1 indicates choosing B, 0 indicates choosing A.
    - A_feature1..4, B_feature1..4: arrays of 0/1 expert ratings for options A and B per trial.
    - parameters: iterable of length 7 as described above.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    p_ttb, flip, start_high, asp, biasB, lapse, beta = parameters

    validities = np.array([0.9, 0.8, 0.7, 0.6])
    n = len(decisions)
    decisions = np.asarray(decisions).astype(int)

    A = np.vstack([A_feature1, A_feature2, A_feature3, A_feature4]).T.astype(float)
    B = np.vstack([B_feature1, B_feature2, B_feature3, B_feature4]).T.astype(float)

    # Compensatory value difference (B - A), validity-weighted
    comp_weights = validities
    dv_comp = (B - A) @ comp_weights  # shape (n,)

    # TTB value difference: simulate misperception and search order deterministically via expected sign
    # Effective perceived cue difference is attenuated toward 0 by flip:
    # With probability flip, a cue is flipped; expected difference becomes (1 - 2*flip) * (B - A)
    atten = (1.0 - 2.0 * flip)
    diff = atten * (B - A)  # expected signed difference per cue in [-1,1]

    # Determine scan order per trial with probability start_high. Use expected DV over random order choice.
    order_high = np.array([0, 1, 2, 3])
    order_low = np.array([3, 2, 1, 0])

    # Compute first-discriminating-cue DV for both orders
    def first_discriminating_dv(dmat, order):
        # dmat shape (n,4), order shape (4,)
        dv = np.zeros(dmat.shape[0])
        for idx in order:
            # A cue discriminates if d != 0. The expected sign is just the value; magnitude scaled by validity.
            mask = (dv == 0) & (np.abs(dmat[:, idx]) > 0)
            # Contribution is sign of difference times validity, scaled by aspiration
            dv[mask] = np.sign(dmat[mask, idx]) * validities[idx] * asp
        # If no discriminating cue, dv remains 0
        return dv

    dv_ttb_high = first_discriminating_dv(diff, order_high)
    dv_ttb_low = first_discriminating_dv(diff, order_low)
    dv_ttb = start_high * dv_ttb_high + (1.0 - start_high) * dv_ttb_low

    # Mixture of TTB and compensatory
    dv = p_ttb * dv_ttb + (1.0 - p_ttb) * dv_comp

    # Side bias centered to [-1,1]
    bias_term = (biasB - 0.5) * 2.0
    dv = dv + bias_term

    # Softmax to probability of choosing B
    pB_model = 1.0 / (1.0 + np.exp(-beta * dv))

    # Lapse mixture
    pB = lapse * 0.5 + (1.0 - lapse) * pB_model
    pB = np.clip(pB, 1e-9, 1.0 - 1e-9)

    ll = decisions * np.log(pB) + (1 - decisions) * np.log(1.0 - pB)
    return -np.sum(ll)


def cognitive_model2(decisions, A_feature1, A_feature2, A_feature3, A_feature4,
                     B_feature1, B_feature2, B_feature3, B_feature4, parameters):
    """Bayesian evidence integration with subjective reliability transformation, loss aversion against B, saturation, bias, and lapse.

    Cognitive idea:
    - Each cue provides independent evidence via its validity v in favor of the option that has the positive rating (1).
      We compute a log-likelihood ratio (LLR) across cues.
    - Subjective reliability transformation shrinks validities toward 0.5 and compresses/expands them via an exponent.
    - Evidence that favors A is upweighted relative to evidence that favors B (loss aversion for B).
    - A saturating nonlinearity (tanh) maps LLR into a bounded decision variable.
    - Choices follow a softmax with a baseline side bias and lapse.

    Parameters (in order):
    - eta: [0,1] Reliability exponent; scales distance from 0.5: v_eff = 0.5 + (v - 0.5) * eta.
    - shrink: [0,1] Additional shrinkage toward 0.5 applied to v_eff by convex combination with 0.5.
    - lambda_neg: [0,1] Asymmetry for evidence against B (favoring A): multiply A-favoring evidence by (1 + lambda_neg) and B-favoring by (1 - lambda_neg).
    - kappa_sat: [0,1] Saturation strength applied as dv = tanh(kappa_sat * LLR); kappa=0 -> fully flat, kappa=1 -> strongest saturation.
    - offsetB: [0,1] Baseline side bias toward B (0.5 = no bias); internally centered to [-1,1].
    - lapse: [0,1] Probability of random choice (uniform 0.5), mixed with the model-based probability.
    - beta: [0,10] Inverse temperature for the softmax.

    Inputs:
    - decisions: array-like of 0/1 where 1 indicates choosing B, 0 indicates choosing A.
    - A_feature1..4, B_feature1..4: arrays of 0/1 expert ratings for options A and B per trial.
    - parameters: iterable of length 7 as described above.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    eta, shrink, lambda_neg, kappa_sat, offsetB, lapse, beta = parameters

    base_validities = np.array([0.9, 0.8, 0.7, 0.6])

    # Subjective reliability
    v_eff = 0.5 + (base_validities - 0.5) * eta
    v_eff = shrink * 0.5 + (1.0 - shrink) * v_eff  # further shrink to 0.5

    # Evidence weights per cue as LLR contributions when cue favors one option
    # LLR contribution magnitude for cue i: log(v_i/(1-v_i))
    llr_mag = np.log(v_eff / (1.0 - v_eff))  # shape (4,)

    A = np.vstack([A_feature1, A_feature2, A_feature3, A_feature4]).T.astype(int)
    B = np.vstack([B_feature1, B_feature2, B_feature3, B_feature4]).T.astype(int)
    decisions = np.asarray(decisions).astype(int)

    # For each trial and cue, determine which side is favored by the cue
    # favor_B = 1 if (B=1, A=0); favor_A = 1 if (A=1, B=0); ties contribute 0
    favor_B = ((B == 1) & (A == 0)).astype(float)
    favor_A = ((A == 1) & (B == 0)).astype(float)

    # Apply asymmetry: amplify A-favoring evidence, attenuate B-favoring evidence
    wB = (1.0 - lambda_neg) * llr_mag  # magnitude for B-favoring evidence
    wA = (1.0 + lambda_neg) * llr_mag  # magnitude for A-favoring evidence

    # Total LLR across cues (positive -> evidence for B)
    LLR = (favor_B * wB - favor_A * wA) @ np.ones(4)

    # Saturating nonlinearity
    dv = np.tanh(kappa_sat * LLR)

    # Add side bias centered to [-1,1]
    dv = dv + (offsetB - 0.5) * 2.0

    # Softmax to probability of choosing B, lapse mixture
    pB_model = 1.0 / (1.0 + np.exp(-beta * dv))
    pB = lapse * 0.5 + (1.0 - lapse) * pB_model
    pB = np.clip(pB, 1e-9, 1.0 - 1e-9)

    ll = decisions * np.log(pB) + (1 - decisions) * np.log(1.0 - pB)
    return -np.sum(ll)


def cognitive_model3(decisions, A_feature1, A_feature2, A_feature3, A_feature4,
                     B_feature1, B_feature2, B_feature3, B_feature4, parameters):
    """Contextual quorum-augmented majority: grouped cue weighting with synergy, threshold, bias, and lapse.

    Cognitive idea:
    - The decision maker aggregates a grouped, validity-weighted margin (B minus A), with separate gains for high- vs low-validity cues.
    - A synergy bonus is added when multiple cues align for one option (quorum effect).
    - A quorum threshold parameter shifts the margin required for choosing B.
    - Choices follow a softmax with side bias and lapse.

    Parameters (in order):
    - wH: [0,1] Gain applied to high-validity cues (0.9 and 0.8).
    - wL: [0,1] Gain applied to low-validity cues (0.7 and 0.6).
    - quorum: [0,1] Quorum threshold shifting the required margin toward B; >0.5 demands stronger B support.
    - synergy: [0,1] Bonus added when at least two cues align for one option; increases DV toward the aligned option.
    - side_pref: [0,1] Baseline side bias toward B (0.5 = no bias); internally centered to [-1,1].
    - lapse: [0,1] Probability of random choice (uniform 0.5), mixed with the model-based probability.
    - beta: [0,10] Inverse temperature for the softmax.

    Inputs:
    - decisions: array-like of 0/1 where 1 indicates choosing B, 0 indicates choosing A.
    - A_feature1..4, B_feature1..4: arrays of 0/1 expert ratings for options A and B per trial.
    - parameters: iterable of length 7 as described above.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    wH, wL, quorum, synergy, side_pref, lapse, beta = parameters

    validities = np.array([0.9, 0.8, 0.7, 0.6])
    gains = np.array([wH, wH, wL, wL])  # group gains for high vs low validity cues

    A = np.vstack([A_feature1, A_feature2, A_feature3, A_feature4]).T.astype(float)
    B = np.vstack([B_feature1, B_feature2, B_feature3, B_feature4]).T.astype(float)
    decisions = np.asarray(decisions).astype(int)

    # Base grouped, validity-weighted margin
    cue_weights = validities * gains
    margin = (B - A) @ cue_weights  # positive favors B

    # Synergy bonus based on aligned cues
    favor_B = (B == 1) & (A == 0)
    favor_A = (A == 1) & (B == 0)
    nB = np.sum(favor_B, axis=1)
    nA = np.sum(favor_A, axis=1)

    # Add synergy if at least two cues align for one option; subtract if for the other
    syn_term = synergy * ((nB >= 2).astype(float) - (nA >= 2).astype(float))

    # Quorum threshold shifts required margin: map quorum in [0,1] to threshold in [-C, C]
    # where C is the maximum possible absolute margin given the weights (upper bound to keep scale sensible).
    C = np.sum(np.abs(cue_weights))  # worst-case all four cues favor one side
    tau = (quorum - 0.5) * 2.0 * C  # negative lowers criterion for B, positive raises it

    # Compose decision variable: margin + synergy - threshold + side bias
    dv = margin + syn_term - tau + (side_pref - 0.5) * 2.0

    # Softmax, lapse mixture
    pB_model = 1.0 / (1.0 + np.exp(-beta * dv))
    pB = lapse * 0.5 + (1.0 - lapse) * pB_model
    pB = np.clip(pB, 1e-9, 1.0 - 1e-9)

    ll = decisions * np.log(pB) + (1 - decisions) * np.log(1.0 - pB)
    return -np.sum(ll)