# ğŸ§  GeCCo: Generative Cognitive Model Composer

**Author:** [Milena Rmus](https://github.com/MilenaCCNlab)  
**Last updated:** October 2025

---

## ğŸ“˜ Overview

**GeCCo (Generative Cognitive Composer)** is a research framework for **automatically generating, fitting, and evaluating cognitive models** using large language models (LLMs).

Given behavioral data (e.g., from decision-making tasks), GeCCo prompts an LLM to generate candidate cognitive models as executable Python functions, fits them to data, evaluates them via information criteria (AIC/BIC), and iteratively improves model quality using structured feedback.

---

## ğŸ§© Key Features

- ğŸ§  **LLM-guided model generation** â€” models are produced as interpretable Python functions.
- âš™ï¸ **Flexible configuration** â€” YAML-based configs define task columns, metrics, and data formats.
- ğŸ“Š **Automatic fitting and evaluation** â€” supports multi-start optimization with AIC/BIC scoring.
- ğŸ” **Iterative model search loop** â€” integrates structured or LLM-generated feedback.
- ğŸ§® **Task-agnostic** â€” supports any dataset by specifying relevant input columns.
- ğŸ§± **Modular architecture** â€” clean separation between generation, fitting, and evaluation.

---

## ğŸ“‚ Repository Structure

gecco/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ schema.py             # Config loader + validation
â”‚   â””â”€â”€ two_step.yaml         # Example config for two-step task
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ evaluation.py         # AIC/BIC and related metrics
â”‚   â”œâ”€â”€ data_structures.py    # FitResult and ModelSpec definitions
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ run_fit.py            # Fits an LLM-generated model to data
â”‚   â”œâ”€â”€ model_search.py       # Iterative search loop for generating and evaluating models
â”‚   â””â”€â”€ feedback.py           # (Optional) custom feedback logic for LLM prompts
â”‚
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ generator.py          # Handles LLM prompting and text generation
â”‚   â””â”€â”€ prompt_builder.py     # Builds task- and iteration-specific prompts
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ extraction.py         # Extracts code blocks, parameter names, and bounds from LLM output
â”‚   â””â”€â”€ misc.py               # Misc. utilities (safe exec, logging, etc.)
â”‚
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ two_step_demo.py      # Example script showing full GeCCo workflow
â”‚
â””â”€â”€ results/
    â”œâ”€â”€ models/               # Saved model definitions per iteration
    â””â”€â”€ bics/                 # BIC results for each model


## âš™ï¸ Configuration

All experiment and data parameters are specified in a YAML file (e.g., `config/two_step.yaml`):

```yaml
data:
  id_column: participant
  input_columns: [action_1, state, action_2, reward]

evaluation:
  metric: BIC
  n_starts: 10

loop:
  max_iterations: 5

llm:
  models_per_iteration: 3
```
## ğŸš€ Usage

1. **Prepare your dataset**  
   Your input DataFrame must contain all columns listed under `data.input_columns` in the config.

2. **Run the demo script**
   ```bash
   cd gecco/examples
   python two_step_demo.py
    ```

## ğŸ§ª How It Works

- **Prompting** â€“ A structured prompt (defined in `prompt_builder.py`) is sent to the LLM, instructing it to generate 1â€“3 candidate model functions (`cognitive_model1`, `cognitive_model2`, ...).

- **Code Extraction** â€“ GeCCo extracts clean function definitions, parameter names, and parameter bounds from the modelâ€™s docstring using regex.

- **Model Fitting** â€“ Each model is compiled and fit to each participantâ€™s data using maximum likelihood estimation via `scipy.optimize.minimize`.

- **Evaluation** â€“ Fit quality is assessed using the metric specified in the config (default: BIC).

- **Feedback Loop** â€“ The system optionally provides feedback to the LLM about which models performed best or which parameter sets to avoid in subsequent iterations.

---

## ğŸ’¬ Customizing Feedback

**Examples include:**
- **LLM-generated feedback**
- **Rule-based feedback**
- **User-defined textual hints**

Then import and plug into the main loop in `model_search.py`.

---

## ğŸ§° Requirements

- **Python â‰¥ 3.10**
- `torch`, `transformers`, `numpy`, `pandas`, `scipy`
- *Optional:* `vllm`, `unsloth`, `accelerate` for large-model inference



